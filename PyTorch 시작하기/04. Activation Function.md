## 활성함수란?
![신경망 연산 과정](/PyTorch%20시작하기/images/04.%20Neural%20Network%20Computation.png)
`활성 함수`는 신경망의 깊은 층에서 신호를 효과적으로 전달하기 위해 사용됩니다. 여기서 **효과적**이라는 것은 신속하고 정확한 학습과 패턴 추적을 의미합니다. 신경망의 층이 깊어질수록 각 뉴런은 더 정밀한 특징을 학습하여 효과가 극대화됩니다.


특히 **출력층에 가까운 층에서는 이전 신호를 `종합`하여 활성 함수를 적용**하므로, 보다 정밀한 특징을 추출하고 예외적인 상황을 감지할 수 있습니다. 활성 함수는 신호를 활성화하는 역할을 하며, **특정 임계치를 초과하면 신호의 강도가 증가**합니다. 또한, 신경망의 각 층마다 적절한 활성 함수를 개별적으로 적용할 수 있습니다.

![ReLU](/PyTorch%20시작하기/images/04.%20ReLu.png)
우리는 대표적으로 `ReLU(Rectified Linear Unit)`을 많이 볼 수 있습니다. ReLU는 0이하의 값은 0으로 처리하고, 0보다 큰 값은 그대로 유지하는 방식입니다. 그런데 왜 이런 그래프를 사용해야할까요? 직선은 활성화함수가 될 수 없을까요?

## 활성함수가 선형함수일 때 발생하는 문제점 
![problem](/PyTorch%20시작하기/images/04.%20Linear%20Function%20Problem.png)

활성함수가 `선형 함수`인 경우, 계산식에서 볼 수 있는 것처럼 **몇 개의 레이어를 쌓던 결국 하나의 뉴런에서 하나의 가중치랑 하나의 바이어스로 계산한 것과 동일**합니다. 레이어를 많이 쌓을수록 많은 연산 시간이 요구되기때문에 선형함수를 사용할 경우, 오히려 **비효율적인** 결과를 가져옵니다. 따라서 레이어를 많이 쌓아서 딥러닝의 효과를 발휘하기위해서는 활성화함수로 비선형함수를 선택해야합니다.

## 활성함수의 종류
![activation functions](/PyTorch%20시작하기/images/04.%20Activation%20Functions.png)


| 활성화 함수  | 특징 |
|-------------|----------------------------------------------------------------|
| **ReLU (Rectified Linear Unit)** | 0 이하의 값은 0으로 처리하고, 0보다 큰 값은 그대로 유지합니다.<br>**0 이하의 값에서 기울기가 0**이 됩니다.|
| **Leaky ReLU** | **0 이하의 입력에서도 작은 음수 값을 출력**하여 완전히 0이 되지 않도록 합니다.|
| **ELU (Exponential Linear Unit)** | **0 이하의 값에서도 지수 함수 형태**를 띄어, 완만하게 음수 값을 가집니다.<br>Leaky ReLU보다 더 연속적이며, Gradient Descent 과정에서 더 효과적인 학습이 가능합니다.|
| **Sigmoid** | **끝부분에서 기울기가 0에 수렴**합니다.<br>**y값이 항상 양수**이므로, 0을 중심으로 하는 데이터 표현이 어렵습니다.|
| **Tanh** | Sigmoid와 비슷하지만, **y값이 음수**가 나올 수 있습니다.<br>0일 때 출력도 0입니다. <br>**끝부분에서 기울기가 0에 수렴**합니다. |
| **Step** | 불연속적입니다.<br>**기울기가 0이므로 역전파가 불가능**하여 딥러닝에서는 사용되지 않습니다. |



## Gradient Vanishing 문제
`순전파(Feedforward)` 연산은 모델이 입력값을 받아 출력값을 **예측하는** 과정입니다. 이 과정에서 입력 데이터를 가중치와 편향을 적용하여 변환하고, 최종적으로 출력층에서 예측값을 도출하게 됩니다. **예측된 값과 실제 값 사이의 차이**를 `오차`라고 하며, 이 **오차를 줄이는 것이 학습의 목표**입니다.

오차를 바탕으로 **모델의 가중치와 편향을 재보정**시켜주는 단계를 `역전파(Backward)` 라고 합니다. **모델 학습과 패턴인식의 과정**입니다. **출력층에서 입력층 방향**으로 오차를 전파하며, 파라미터들을 재조정합니다. 역전파는 `미분`을 이용하여 변화량을 측정하는 원리로 작동합니다. 즉, **특정 뉴런의 가중치를 조정했을 때 오차가 얼마나 변하는지를 계산**하여, 각 뉴런이 결과에 미치는 영향을 평가하는 것입니다. 

여기서부터 `기울기(Gradient)`가 등장합니다. 이 과정에서 등장하는 기울기는 **손실함수(Loss Function)를 가중치(Weight)로 미분한 값**을 말합니다. 즉, 가중치가 변화할 때 오차가 얼마나 변하는지를 나타내는 척도입니다. 


* 기울기가 양수이면 가중치를 낮추려는 방향으로 조정합니다.
* 기울기가 음수이면 가중치를 높이는 방향으로 조정합니다.
* 기울기가 클수록 가중치의 변화 폭이 커집니다.

이때, **가중치를 조정하는 강도를 조절**하는 하이퍼파라미터가 `학습률(Learning Rate)`입니다. 

![Gradient Vanishing](/PyTorch%20시작하기/images/04.%20Gradient%20Vanishing.png)


**Gradient Vanishing 문제는** `역전파 과정`에서 `기울기`**가 점점 0에 가까워지는 현상**을 말합니다. `Sigmoid`나 `Tanh`같은 **활성화함수**를 쓰게되면, 입력값이 매우 크거나 작으면 **기울기가 0으로 수렴하는 것**을 볼 수 있습니다. 이렇게 되면 역전파 과정에서 기울기 전파가 점점 약해지게 됩니다. 그러면, **입력층 가까이에 있는 뉴런들의 가중치가 거의 변하지 않게 되어** forward 예측 연산을 할 때 큰 의미가 없게됩니다. 

이러한 문제를 해결하기 위해 `ReLU(Rectified Linear Unit)` 활성화 함수가 등장하였습니다. ReLU를 사용하면, 역전파 시 **기울기가 1이상 유지되는 뉴런들이 존재**하기 때문에 이전 층까지 충분한 신호를 전달할 수 있어 **기울기 소실문제를 완화**할 수 있수 있습니다. ReLU의 단점 중 하나는 **입력값이 0 이하일 경우 기울기가 완전히 0이 되어** 더이상 뉴런이 반응하지않는 문제가 발생할 수 있다는 것입니다. 이런 ReLU의 문제점을 해결하여 Leaky ReLU, ELU가 등장했습니다. 
